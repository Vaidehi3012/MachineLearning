{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision Trees, SVM, and Naive Bayes"
      ],
      "metadata": {
        "id": "HIt8hBSq90lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "  - Information Gain (IG) is a key concept in decision tree algorithms, particularly in classification problems. It measures how much \"information\" a feature gives us about the class labels — in other words, how well a feature separates the data into different classes.\n",
        "\n",
        "Here’s a detailed explanation:\n",
        "\n",
        "1. Entropy: The Basis of Information Gain\n",
        "\n",
        "Entropy is a measure of uncertainty or disorder in a dataset. For a dataset\n",
        "𝐷\n",
        "D with classes\n",
        "𝐶\n",
        "1\n",
        ",\n",
        "𝐶\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝐶\n",
        "𝑛\n",
        "C\n",
        "1\n",
        "\t​\n",
        "\n",
        ",C\n",
        "2\n",
        "\t​\n",
        "\n",
        ",…,C\n",
        "n\n",
        "\t​\n",
        "\n",
        ", the entropy is calculated as:\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "H(D)=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " = proportion of instances in class\n",
        "𝐶\n",
        "𝑖\n",
        "C\n",
        "i\n",
        "\t​\n",
        "\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "H(D) is maximum when classes are evenly distributed (most uncertain)\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "0\n",
        "H(D)=0 when all instances belong to one class (completely certain)\n",
        "\n",
        "2. Information Gain Formula\n",
        "\n",
        "Information Gain measures the reduction in entropy after splitting the dataset based on a feature\n",
        "𝐴\n",
        "A:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "(\n",
        "𝐷\n",
        ",\n",
        "𝐴\n",
        ")\n",
        "=\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑣\n",
        "∈\n",
        "Values\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "∣\n",
        "𝐷\n",
        "𝑣\n",
        "∣\n",
        "∣\n",
        "𝐷\n",
        "∣\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        "𝑣\n",
        ")\n",
        "IG(D,A)=H(D)−\n",
        "v∈Values(A)\n",
        "∑\n",
        "\t​\n",
        "\n",
        "∣D∣\n",
        "∣D\n",
        "v\n",
        "\t​\n",
        "\n",
        "∣\n",
        "\t​\n",
        "\n",
        "H(D\n",
        "v\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "H(D) = entropy of the original dataset\n",
        "\n",
        "𝐷\n",
        "𝑣\n",
        "D\n",
        "v\n",
        "\t​\n",
        "\n",
        " = subset of\n",
        "𝐷\n",
        "D where feature\n",
        "𝐴\n",
        "A takes value\n",
        "𝑣\n",
        "v\n",
        "\n",
        "∣\n",
        "𝐷\n",
        "𝑣\n",
        "∣\n",
        "/\n",
        "∣\n",
        "𝐷\n",
        "∣\n",
        "∣D\n",
        "v\n",
        "\t​\n",
        "\n",
        "∣/∣D∣ = weight of the subset in the total dataset\n",
        "\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        "𝑣\n",
        ")\n",
        "H(D\n",
        "v\n",
        "\t​\n",
        "\n",
        ") = entropy of the subset after the split\n",
        "\n",
        "So, Information Gain tells us how much uncertainty is reduced by knowing the value of feature\n",
        "𝐴\n",
        "A.\n",
        "\n",
        "3. Use in Decision Trees\n",
        "\n",
        "Decision tree algorithms (like ID3 and C4.5) use Information Gain to choose the best feature to split the data at each node:\n",
        "\n",
        "Calculate the entropy of the dataset.\n",
        "\n",
        "For each feature, compute the weighted entropy after splitting.\n",
        "\n",
        "Subtract this from the original entropy to get the information gain.\n",
        "\n",
        "Select the feature with highest Information Gain as the splitting node.\n",
        "\n",
        "Repeat recursively for child nodes until stopping criteria are met.\n",
        "\n",
        "This ensures that each split maximally reduces uncertainty and improves classification.\n",
        "\n",
        "4. Intuitive Example\n",
        "\n",
        "Suppose we are predicting whether someone will play tennis based on weather:\n",
        "\n",
        "Outlook\tPlayTennis\n",
        "Sunny\tNo\n",
        "Sunny\tNo\n",
        "Overcast\tYes\n",
        "Rain\tYes\n",
        "Rain\tNo\n",
        "\n",
        "Entropy of the full dataset\n",
        "𝐻\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "H(D) measures uncertainty about \"PlayTennis\".\n",
        "\n",
        "Splitting on \"Outlook\" reduces entropy differently for Sunny, Overcast, Rain.\n",
        "\n",
        "IG for \"Outlook\" is calculated.\n",
        "\n",
        "The feature with the highest IG becomes the root of the decision tree.\n",
        "\n",
        "✅ Key Takeaways:\n",
        "\n",
        "Information Gain = reduction in uncertainty after splitting on a feature.\n",
        "\n",
        "Used to select features in decision trees.\n",
        "\n",
        "High IG → feature is good at classifying data; low IG → feature is less useful."
      ],
      "metadata": {
        "id": "5XNZ9JIP901O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Gini Impurity and Entropy?\n",
        "  - | Feature                | **Entropy**                                                                                       | **Gini Impurity**                                                                       |\n",
        "| ---------------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\n",
        "| **Definition**         | Measures the **uncertainty** in a dataset. Formula: ( H(D) = -\\sum_{i} p_i \\log_2 p_i )           | Measures the **probability of misclassification**. Formula: ( G(D) = 1 - \\sum_i p_i^2 ) |\n",
        "| **Range**              | 0 (pure) to (\\log_2(n)) (maximum uncertainty)                                                     | 0 (pure) to (1 - 1/n) (maximum impurity)                                                |\n",
        "| **Interpretation**     | How mixed the classes are; higher → more uncertainty                                              | Chance of incorrectly classifying a randomly chosen instance; higher → more impurity    |\n",
        "| **Computational Cost** | Uses log function → slightly more expensive                                                       | Only involves squaring probabilities → faster to compute                                |\n",
        "| **Sensitivity**        | More sensitive to changes in class probabilities, especially when probabilities are near 0 or 1   | Less sensitive; tends to produce similar splits as entropy                              |\n",
        "| **Use Cases**          | ID3 algorithm typically uses Entropy                                                              | CART algorithm typically uses Gini Impurity                                             |\n",
        "| **Strengths**          | Gives a more “information-theoretic” view of splits; works well when class distribution is uneven | Simpler and faster to compute; often yields similar results to entropy                  |\n"
      ],
      "metadata": {
        "id": "Mj7jmgUG91Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Pre-Pruning in Decision Trees?\n",
        "  - Pre-Pruning in decision trees is a technique used to stop the tree from growing too complex before it fully develops. It’s essentially “cutting the tree early” to prevent overfitting.\n",
        "\n",
        "Here’s a detailed explanation:\n",
        "\n",
        "1. Definition\n",
        "\n",
        "Pre-pruning (also called early stopping) is when the algorithm halts the growth of a decision tree during training based on certain criteria, rather than growing it fully and pruning later.\n",
        "\n",
        "2. How It Works\n",
        "\n",
        "During tree construction, the algorithm evaluates whether a node should be split further. If a stopping criterion is met, the node becomes a leaf. Common pre-pruning conditions include:\n",
        "\n",
        "Maximum depth: Stop if the tree reaches a certain depth.\n",
        "\n",
        "Minimum samples per node: Stop if a node has fewer than a threshold number of samples.\n",
        "\n",
        "Minimum information gain / impurity reduction: Stop if the best split doesn’t significantly reduce impurity (Entropy or Gini).\n",
        "\n",
        "Maximum number of nodes: Limit the total number of nodes.\n",
        "\n",
        "3. Purpose\n",
        "\n",
        "Prevent overfitting: Stops the tree from modeling noise in the training data.\n",
        "\n",
        "Reduce complexity: Produces smaller, more interpretable trees.\n",
        "\n",
        "Improve generalization: Helps the model perform better on unseen data.\n",
        "\n",
        "4. Pros and Cons\n",
        "Pros\tCons\n",
        "Faster to train\tMay underfit if stopped too early\n",
        "Produces simpler trees\tChoosing optimal thresholds for stopping can be tricky\n",
        "Reduces risk of overfitting\tMight miss important splits\n",
        "5. Example\n",
        "\n",
        "Suppose you are building a tree to predict loan defaults:\n",
        "\n",
        "Without pre-pruning: Tree splits until each leaf has only 1 sample → overfitting\n",
        "\n",
        "With pre-pruning: Stop splitting if a node has fewer than 10 samples or if maximum depth = 5 → simpler tree, better generalization"
      ],
      "metadata": {
        "id": "nCHootVw91Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Gini Impurity\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i} ({iris.feature_names[i]}): {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc-JPtq4_mUP",
        "outputId": "758578b5-67f0-40a5-c79b-c88b93dc3c97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "Feature 0 (sepal length (cm)): 0.0000\n",
            "Feature 1 (sepal width (cm)): 0.0167\n",
            "Feature 2 (petal length (cm)): 0.9061\n",
            "Feature 3 (petal width (cm)): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is a Support Vector Machine (SVM)?\n",
        "  - A Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification (and sometimes regression). It works by finding the best boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "Here’s a detailed breakdown:\n",
        "\n",
        "1. Core Idea\n",
        "\n",
        "SVM tries to find a hyperplane that divides the dataset into classes.\n",
        "\n",
        "The margin is the distance between the hyperplane and the nearest data points of each class (called support vectors).\n",
        "\n",
        "SVM aims to maximize this margin to improve generalization.\n",
        "\n",
        "2. Hyperplanes\n",
        "\n",
        "In 2D: the hyperplane is a line separating two classes.\n",
        "\n",
        "In 3D: it’s a plane.\n",
        "\n",
        "In higher dimensions: it’s called a hyperplane.\n",
        "\n",
        "3. Linear vs. Non-linear SVM\n",
        "\n",
        "Linear SVM: Data is linearly separable; a straight line (or hyperplane) separates classes.\n",
        "\n",
        "Non-linear SVM: Uses a kernel trick to map data into higher-dimensional space where it becomes linearly separable. Common kernels:\n",
        "\n",
        "Polynomial\n",
        "\n",
        "Radial Basis Function (RBF) / Gaussian\n",
        "\n",
        "Sigmoid\n",
        "\n",
        "4. Support Vectors\n",
        "\n",
        "These are the data points closest to the hyperplane.\n",
        "\n",
        "They are critical because they define the position and orientation of the hyperplane.\n",
        "\n",
        "5. Advantages\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "\n",
        "Works well with clear margin of separation.\n",
        "\n",
        "Memory-efficient (only support vectors matter, not all data points).\n",
        "\n",
        "6. Limitations\n",
        "\n",
        "Not suitable for very large datasets (training can be slow).\n",
        "\n",
        "Less effective when data is noisy and overlapping.\n",
        "\n",
        "Choice of kernel and parameters is important for good performance."
      ],
      "metadata": {
        "id": "gp37oD2w91ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the Kernel Trick in SVM?\n",
        "  - The Kernel Trick in Support Vector Machines (SVM) is a clever technique that allows SVM to perform non-linear classification without explicitly transforming data into a higher-dimensional space.\n",
        "\n",
        "Here’s a detailed explanation:\n",
        "\n",
        "1. The Problem\n",
        "\n",
        "Standard SVM works well when data is linearly separable.\n",
        "\n",
        "For non-linear data, you need to map it to a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Explicitly computing this mapping can be computationally expensive or even infeasible.\n",
        "\n",
        "2. The Solution: Kernel Trick\n",
        "\n",
        "Instead of explicitly mapping data to a higher dimension, the kernel trick computes the dot product of data points in that higher-dimensional space directly.\n",
        "\n",
        "This allows SVM to find a linear hyperplane in the transformed space, which corresponds to a non-linear decision boundary in the original space.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "If\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x) is the transformation function, a kernel function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ") satisfies:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "⋅\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=ϕ(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ")⋅ϕ(x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")\n",
        "3. Common Kernel Functions\n",
        "Kernel\tFormula\tUse Case\n",
        "Linear\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=x\n",
        "i\n",
        "\t​\n",
        "\n",
        "⋅x\n",
        "j\n",
        "\t​\n",
        "\n",
        "\tLinearly separable data\n",
        "Polynomial\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "⋅x\n",
        "j\n",
        "\t​\n",
        "\n",
        "+c)\n",
        "d\n",
        "\tCaptures polynomial relationships\n",
        "RBF / Gaussian\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∥\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∥\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=exp(−γ∥x\n",
        "i\n",
        "\t​\n",
        "\n",
        "−x\n",
        "j\n",
        "\t​\n",
        "\n",
        "∥\n",
        "2\n",
        ")\tNon-linear, flexible decision boundaries\n",
        "Sigmoid\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "tanh\n",
        "⁡\n",
        "(\n",
        "𝛼\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=tanh(αx\n",
        "i\n",
        "\t​\n",
        "\n",
        "⋅x\n",
        "j\n",
        "\t​\n",
        "\n",
        "+c)\tSimilar to neural network activation\n",
        "4. Intuition\n",
        "\n",
        "Think of kernel trick as magically bending the feature space so that a straight line (hyperplane) can separate the data in this new space.\n",
        "\n",
        "In the original space, this separation appears as a curved boundary.\n",
        "\n",
        "5. Why It’s Useful\n",
        "\n",
        "Avoids explicit computation in high-dimensional space.\n",
        "\n",
        "Makes SVM efficient and powerful for complex, non-linear problems.\n",
        "\n",
        "Enables SVM to handle a wide variety of classification tasks."
      ],
      "metadata": {
        "id": "Pb-IN23k91bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Initialize and train the SVM classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for both classifiers\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZjlQLAbBHE9",
        "outputId": "5f54fbb6-3603-4ab1-8e8f-e341b90855d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 1.0000\n",
            "Accuracy of SVM with RBF Kernel: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "  - The Naïve Bayes classifier is a probabilistic machine learning algorithm used for classification, based on Bayes’ Theorem. It’s called “naïve” because it makes a strong assumption that all features are independent of each other, which is rarely true in real-world data.\n",
        "\n",
        "Here’s a detailed explanation:\n",
        "\n",
        "1. Bayes’ Theorem\n",
        "\n",
        "Bayes’ Theorem gives the probability of a class\n",
        "𝐶\n",
        "C given features\n",
        "𝑋\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "X=(x\n",
        "1\n",
        "\t​\n",
        "\n",
        ",x\n",
        "2\n",
        "\t​\n",
        "\n",
        ",...,x\n",
        "n\n",
        "\t​\n",
        "\n",
        "):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)⋅P(C)\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X) = probability of class\n",
        "𝐶\n",
        "C given the features\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C) = probability of features given the class\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C) = prior probability of class\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X) = probability of features (normalization factor)\n",
        "\n",
        "2. The “Naïve” Assumption\n",
        "\n",
        "Naïve Bayes assumes features are independent given the class, i.e.,\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        "𝑗\n",
        ",\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣x\n",
        "j\n",
        "\t​\n",
        "\n",
        ",C)=P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣C).\n",
        "\n",
        "This simplifies calculation:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "≈\n",
        "∏\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C)=P(x\n",
        "1\n",
        "\t​\n",
        "\n",
        ",x\n",
        "2\n",
        "\t​\n",
        "\n",
        ",...,x\n",
        "n\n",
        "\t​\n",
        "\n",
        "∣C)≈\n",
        "i=1\n",
        "∏\n",
        "n\n",
        "\t​\n",
        "\n",
        "P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣C)\n",
        "\n",
        "Without this assumption, computing\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C) would be computationally expensive for many features.\n",
        "\n",
        "3. How It Works\n",
        "\n",
        "Compute prior probabilities for each class\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C).\n",
        "\n",
        "Compute likelihood of each feature given the class\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(x\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣C).\n",
        "\n",
        "Use Bayes’ Theorem to compute posterior probability\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X) for each class.\n",
        "\n",
        "Assign the sample to the class with highest posterior probability.\n",
        "\n",
        "4. Types of Naïve Bayes\n",
        "\n",
        "Gaussian NB: For continuous data (assumes Gaussian distribution).\n",
        "\n",
        "Multinomial NB: For count-based data (like text data, word frequencies).\n",
        "\n",
        "Bernoulli NB: For binary features (presence/absence of a feature).\n",
        "\n",
        "5. Why It Works Despite “Naïve” Assumption\n",
        "\n",
        "Even though features are often correlated, Naïve Bayes performs surprisingly well in practice, especially in text classification, spam detection, and sentiment analysis.\n",
        "\n",
        "Its simplicity makes it fast and effective, even for high-dimensional data."
      ],
      "metadata": {
        "id": "DQjXTS4u92Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "  - | Feature                       | **Gaussian Naïve Bayes (GNB)**                                       | **Multinomial Naïve Bayes (MNB)**                                     | **Bernoulli Naïve Bayes (BNB)**                                           |     |                                                          |\n",
        "| ----------------------------- | -------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------- | --- | -------------------------------------------------------- |\n",
        "| **Data type**                 | Continuous numeric features                                          | Count data (non-negative integers)                                    | Binary features (0/1, yes/no, presence/absence)                           |     |                                                          |\n",
        "| **Assumption about features** | Each feature follows a **Gaussian (normal) distribution**            | Features are counts of events and follow **multinomial distribution** | Features are **Bernoulli-distributed** (true/false)                       |     |                                                          |\n",
        "| **Common use cases**          | Predicting continuous attributes like age, height, or temperature    | Text classification using word counts (e.g., bag-of-words model)      | Text classification with binary occurrence of words (word present or not) |     |                                                          |\n",
        "| **Likelihood computation**    | Uses **mean and variance** to compute (P(x_i                         | C)) for continuous values                                             | Uses **frequency counts** to compute (P(x_i                               | C)) | Uses **presence/absence** probabilities for each feature |\n",
        "| **Example**                   | Predicting whether a patient has a disease based on lab measurements | Classifying emails as spam based on word counts                       | Classifying emails as spam based on whether certain keywords appear       |     |                                                          |\n"
      ],
      "metadata": {
        "id": "SqXy4eFLBz3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Breast Cancer Dataset. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes Classifier: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pk9zgpUB0jg",
        "outputId": "864f25d5-ce6d-4c1f-c710-7dcaed9e59ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier: 0.9737\n"
          ]
        }
      ]
    }
  ]
}